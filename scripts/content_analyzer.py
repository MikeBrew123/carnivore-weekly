#!/usr/bin/env python3
"""
Content Analyzer for Carnivore Diet YouTube Data

This script uses Claude AI (Anthropic API) to analyze the collected YouTube data
and generate insights, summaries, and trending topics for the weekly update.

Author: Created with Claude Code
Date: 2025-12-26
"""

import os
import json
from datetime import datetime
from pathlib import Path
from typing import Dict, List

# Third-party imports
from dotenv import load_dotenv
from anthropic import Anthropic

# Load environment variables
load_dotenv()

# ============================================================================
# CONFIGURATION
# ============================================================================

ANTHROPIC_API_KEY = os.getenv('ANTHROPIC_API_KEY')
PROJECT_ROOT = Path(__file__).parent.parent
DATA_DIR = PROJECT_ROOT / 'data'
INPUT_FILE = DATA_DIR / 'youtube_data.json'
OUTPUT_FILE = DATA_DIR / 'analyzed_content.json'

# Claude model to use
CLAUDE_MODEL = "claude-sonnet-4-5-20250929"


# ============================================================================
# CONTENT ANALYZER CLASS
# ============================================================================

class ContentAnalyzer:
    """
    Analyzes carnivore diet YouTube content using Claude AI
    """

    def __init__(self, api_key: str):
        """Initialize the Anthropic client"""
        if not api_key:
            raise ValueError(
                "Anthropic API key not found! "
                "Please set ANTHROPIC_API_KEY in your .env file"
            )

        self.client = Anthropic(api_key=api_key)
        print("‚úì Claude AI client initialized")


    def load_youtube_data(self, input_file: Path) -> Dict:
        """Load the YouTube data from JSON file"""
        print(f"\nüìÇ Loading YouTube data from: {input_file}")

        if not input_file.exists():
            raise FileNotFoundError(
                f"YouTube data file not found: {input_file}\n"
                "Please run youtube_collector.py first!"
            )

        with open(input_file, 'r', encoding='utf-8') as f:
            data = json.load(f)

        print(f"‚úì Loaded data for {len(data['top_creators'])} creators")
        return data


    def create_analysis_prompt(self, youtube_data: Dict) -> str:
        """
        Create a detailed prompt for Claude to analyze the YouTube data
        """
        # Extract key information for the prompt
        creators = youtube_data['top_creators']
        total_videos = sum(len(c['videos']) for c in creators)

        # Build a summary of all videos and comments
        content_summary = []

        for creator in creators:
            creator_info = f"\n## {creator['channel_name']}\n"
            creator_info += f"Channel ID: {creator['channel_id']}\n"
            creator_info += f"Total views (past week): {creator['total_views_week']:,}\n"
            creator_info += f"Videos ({len(creator['videos'])}):\n"

            for video in creator['videos']:
                video_info = f"\n### {video['title']}\n"
                video_info += f"- Video ID: {video['video_id']}\n"
                video_info += f"- Published: {video['published_at']}\n"
                video_info += f"- Views: {video['statistics']['view_count']:,}\n"
                video_info += f"- Likes: {video['statistics']['like_count']:,}\n"
                video_info += f"- Comments: {video['statistics']['comment_count']:,}\n"

                # Add top 5 comments for context
                if video['top_comments']:
                    video_info += "\nTop comments:\n"
                    for i, comment in enumerate(video['top_comments'][:5], 1):
                        video_info += f"{i}. {comment['text'][:100]}...\n"

                creator_info += video_info

            content_summary.append(creator_info)

        # Create the full prompt
        prompt = f"""You are analyzing carnivore diet content from YouTube for a weekly content hub.

I've collected data from the top {len(creators)} carnivore diet creators over the past week ({total_videos} total videos).

Here's the detailed data:

{''.join(content_summary)}

Please provide a comprehensive analysis in JSON format with the following structure:

{{
  "weekly_summary": "A 2-3 paragraph overview of the week's carnivore diet content",
  "trending_topics": [
    {{
      "topic": "Topic name",
      "description": "Why it's trending",
      "mentioned_by": ["Creator 1", "Creator 2"]
    }}
  ],
  "top_videos": [
    {{
      "video_id": "video_id from data",
      "channel_id": "channel_id from data",
      "title": "Video title",
      "creator": "Creator name",
      "views": number,
      "published_at": "published_at from data",
      "why_notable": "What makes this video stand out"
    }}
  ],
  "community_sentiment": {{
    "overall_tone": "positive/neutral/mixed",
    "common_questions": ["Question 1", "Question 2"],
    "success_stories": [
      {{
        "story": "Summary of success story",
        "username": "commenter username from data",
        "video_id": "video_id where comment was found",
        "video_title": "title of video"
      }}
    ]
  }},
  "key_insights": [
    "Insight 1",
    "Insight 2",
    "Insight 3"
  ],
  "recommended_watching": [
    {{
      "video_id": "video_id from data",
      "channel_id": "channel_id from data",
      "title": "Video title",
      "creator": "Creator name",
      "published_at": "published_at from data",
      "reason": "Why viewers should watch this"
    }}
  ]
}}

Focus on:
1. What topics are creators discussing this week?
2. Are there any controversies or debates?
3. What questions is the community asking?
4. Any notable success stories or testimonials?
5. Which videos provide the most value to viewers?

**IMPORTANT**: Make sure to include video_id, channel_id, and published_at (from the data provided above) in top_videos and recommended_watching so we can create clickable links and show publish dates.

Provide actionable, interesting insights that would help someone stay up-to-date with the carnivore diet community."""

        return prompt


    def analyze_content(self, youtube_data: Dict) -> Dict:
        """
        Use Claude AI to analyze the YouTube content
        """
        print("\nü§ñ Analyzing content with Claude AI...")
        print(f"   Model: {CLAUDE_MODEL}")

        # Create the analysis prompt
        prompt = self.create_analysis_prompt(youtube_data)

        # Call Claude API
        try:
            message = self.client.messages.create(
                model=CLAUDE_MODEL,
                max_tokens=4096,
                messages=[
                    {
                        "role": "user",
                        "content": prompt
                    }
                ]
            )

            # Extract the response
            response_text = message.content[0].text

            print("‚úì Analysis complete!")

            # Parse the JSON response
            try:
                analysis = json.loads(response_text)
                return analysis
            except json.JSONDecodeError:
                # If Claude didn't return pure JSON, try to extract it
                print("‚ö† Response wasn't pure JSON, extracting...")
                # Find JSON in the response
                start = response_text.find('{')
                end = response_text.rfind('}') + 1
                if start != -1 and end > start:
                    analysis = json.loads(response_text[start:end])
                    return analysis
                else:
                    raise ValueError("Could not extract JSON from Claude's response")

        except Exception as e:
            print(f"‚úó Error during analysis: {e}")
            raise


    def save_analysis(self, analysis: Dict, youtube_data: Dict, output_file: Path):
        """
        Save the analyzed content to a JSON file
        """
        print(f"\nüíæ Saving analysis to: {output_file}")

        # Combine original data with analysis
        final_data = {
            "analysis_date": datetime.now().strftime('%Y-%m-%d'),
            "analysis_timestamp": datetime.now().isoformat(),
            "source_data": {
                "collection_date": youtube_data['collection_date'],
                "total_creators": youtube_data['top_creators_count'],
                "total_videos": sum(len(c['videos']) for c in youtube_data['top_creators']),
                "search_query": youtube_data['search_query']
            },
            "analysis": analysis,
            "creators_data": youtube_data['top_creators']
        }

        # Create output directory if needed
        output_file.parent.mkdir(parents=True, exist_ok=True)

        # Save to file
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(final_data, f, indent=2, ensure_ascii=False)

        print("‚úì Analysis saved successfully!")
        print(f"\nüìä Analysis Summary:")
        print(f"   - Trending topics: {len(analysis.get('trending_topics', []))}")
        print(f"   - Top videos highlighted: {len(analysis.get('top_videos', []))}")
        print(f"   - Key insights: {len(analysis.get('key_insights', []))}")
        print(f"   - Recommended videos: {len(analysis.get('recommended_watching', []))}")


    def run_analysis(self):
        """
        Main method to run the complete analysis pipeline
        """
        print("\n" + "="*70)
        print("üß† CARNIVORE DIET CONTENT ANALYZER")
        print("="*70)

        try:
            # Load YouTube data
            youtube_data = self.load_youtube_data(INPUT_FILE)

            # Analyze content with Claude
            analysis = self.analyze_content(youtube_data)

            # Save results
            self.save_analysis(analysis, youtube_data, OUTPUT_FILE)

            print("\n" + "="*70)
            print("‚úì ANALYSIS COMPLETE!")
            print("="*70)
            print(f"\nüìÅ Results saved to: {OUTPUT_FILE}")
            print("\nNext step: Run generate_pages.py to create the website!\n")

        except FileNotFoundError as e:
            print(f"\n‚úó Error: {e}")
        except Exception as e:
            print(f"\n‚úó Unexpected error: {e}")
            raise


# ============================================================================
# MAIN EXECUTION
# ============================================================================

def main():
    """Main function"""
    try:
        analyzer = ContentAnalyzer(ANTHROPIC_API_KEY)
        analyzer.run_analysis()

    except ValueError as e:
        print(f"\n‚úó Configuration Error: {e}")
        print("\nPlease check:")
        print("1. ANTHROPIC_API_KEY is set in your .env file")
        print("2. Your API key is valid")
        print("3. You have credits available in your Anthropic account")

    except KeyboardInterrupt:
        print("\n\n‚ö† Analysis interrupted by user")

    except Exception as e:
        print(f"\n‚úó Unexpected error: {e}")
        raise


if __name__ == '__main__':
    main()
